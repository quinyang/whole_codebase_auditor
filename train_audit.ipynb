{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b86fbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/quinyang/whole_codebase_auditor/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "# Import both functions from your parser\n",
    "from tree_parser import repo_scan_parser, build_mamba_prompt\n",
    "from pure_mamba_audit import PureMambaAuditor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13bbc183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_ID = \"state-spaces/mamba-2.8b-hf\" \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e19fd726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Initializing Pure Mamba WCA on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [05:41<00:00, 113.98s/it]\n"
     ]
    }
   ],
   "source": [
    "auditor = PureMambaAuditor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba51ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Repo Scan...\n",
      "üîπ Authenticate with GitHub...\n",
      "üîπ Fetching file list from branch: V2.1...\n",
      "Found 15 files in the repository.\n",
      "\n",
      "‚è≠Ô∏è Skipping unsupported file: .env\n",
      "‚è≠Ô∏è Skipping unsupported file: .gitignore\n",
      "‚úÖ Loaded grammar: CPP\n",
      "üîπ Processing file (3/15): HPC/heston_gpu.cu\n",
      "‚úÖ Loaded grammar: PYTHON\n",
      "üîπ Processing file (4/15): HPC/heston_server.py\n",
      "üîπ Processing file (5/15): HPC/stock_calibration.py\n",
      "üîπ Processing file (6/15): ML/generate_training_data.py\n",
      "üîπ Processing file (7/15): ML/improved_parameter_variation.py\n",
      "üîπ Processing file (8/15): ML/ml_data_loader.py\n",
      "üîπ Processing file (9/15): ML/ml_model.py\n",
      "‚è≠Ô∏è Skipping unsupported file: ML/mlp.ipynb\n",
      "‚è≠Ô∏è Skipping unsupported file: README.md\n",
      "‚è≠Ô∏è Skipping known massive vendor file: include/json.hpp\n",
      "‚è≠Ô∏è Skipping unsupported file: requirements.txt\n",
      "‚è≠Ô∏è Skipping unsupported file: training_data/metadata.json\n",
      "‚è≠Ô∏è Skipping unsupported file: training_data/normalization_stats.json\n",
      "\n",
      "‚úÖ Scan Complete. Processed 7 files.\n",
      "üìä Total Codebase Size: 90.91 KB\n"
     ]
    }
   ],
   "source": [
    "# 1. Scan Files (Using your existing code)\n",
    "print(\"üöÄ Starting Repo Scan...\")\n",
    "scanned_files = repo_scan_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c8c208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîó Stitching Mamba Context...\n",
      "üì¶ Prepared Prompt Length: 93,834 characters\n"
     ]
    }
   ],
   "source": [
    "# 2. Build the Giant Prompt (Using your existing code)\n",
    "# This automatically handles skipping json.hpp, adding XML tags, and appending the Query.\n",
    "print(\"\\nüîó Stitching Mamba Context...\")\n",
    "huge_prompt = build_mamba_prompt(scanned_files)\n",
    "\n",
    "print(f\"üì¶ Prepared Prompt Length: {len(huge_prompt):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1731452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Tokenizing entire codebase...\n",
      "üìä Total Input Size: 27,741 tokens\n",
      "üïµÔ∏è‚Äç‚ôÇÔ∏è WCA is auditing the entire repo in one pass... (Hold tight)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "# 3. Run Inference\n",
    "result = auditor.audit_codebase(huge_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c88cbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üõ°Ô∏è  PURE MAMBA AUDIT REPORT\n",
      "==================================================\n",
      "hi saidPrep sense IBl...\n",
      "\n",
      "hpry page?\n",
      "\n",
      "\n",
      "##\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üõ°Ô∏è  PURE MAMBA AUDIT REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
